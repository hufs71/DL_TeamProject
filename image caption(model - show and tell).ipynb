{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JieUf9PD9K6u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6NB7sjGecgi",
        "outputId": "a43757a9-b885-4483-b2b7-4195e51e3a87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 폴더와 캡션 폴더 경로\n",
        "train_images_folder = '/content/drive/MyDrive/이미지 설명문 추출 및 생성용 한국형 비전 데이터/Training/원천데이터'\n",
        "train_captions_folder = '/content/drive/MyDrive/이미지 설명문 추출 및 생성용 한국형 비전 데이터/Training/라벨링데이터'\n",
        "val_images_folder = '/content/drive/MyDrive/이미지 설명문 추출 및 생성용 한국형 비전 데이터/Validation/원천데이터'\n",
        "val_captions_folder = '/content/drive/MyDrive/이미지 설명문 추출 및 생성용 한국형 비전 데이터/Validation/라벨링데이터'"
      ],
      "metadata": {
        "id": "osI-UNTJndJ0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 캡션 파일 형식이 JSON일 때\n",
        "def load_captions_from_json(folder_path):\n",
        "    captions = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.json'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                data = json.load(file)\n",
        "                captions.extend(data.values())\n",
        "    return captions\n"
      ],
      "metadata": {
        "id": "NCqZxJiMncyk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터 및 검증 데이터 로드\n",
        "train_captions = load_captions_from_json(train_captions_folder)\n",
        "val_captions = load_captions_from_json(val_captions_folder)"
      ],
      "metadata": {
        "id": "fwvBcsTjniS7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 특성 추출을 위한 InceptionV3 모델 로드\n",
        "image_model = tf.keras.applications.InceptionV3(weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-2].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MjAG4W9-gD3",
        "outputId": "6a2b6a64-fb5e-41bc-accd-c239476a3663"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 전처리 함수\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    img = image.img_to_array(img)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "metadata": {
        "id": "o8BXNEBdGV-y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지와 캡션 특성 결합을 위한 데이터셋 생성\n",
        "def create_dataset(image_folder, captions, image_features_extract_model, batch_size=32):\n",
        "    image_paths = [os.path.join(image_folder, img_name) for img_name in os.listdir(image_folder)]\n",
        "    images = []\n",
        "    sequences = []\n",
        "\n",
        "    for img_path, caption in tqdm(zip(image_paths, captions), desc=f'Loading captions from {image_folder}'):\n",
        "        img, _ = load_image(img_path)\n",
        "        images.append(img)\n",
        "        sequences.append(caption)\n",
        "\n",
        "        # 배치 크기만큼 데이터가 쌓이면 처리\n",
        "        if len(images) == batch_size:\n",
        "            images = tf.convert_to_tensor(images)\n",
        "            image_features = image_features_extract_model(images)\n",
        "\n",
        "            # 여기서 image_features를 활용하여 다른 작업 수행\n",
        "\n",
        "            # 초기화\n",
        "            images = []\n",
        "            sequences = []\n",
        "\n",
        "    # 마지막으로 쌓인 데이터 처리\n",
        "    if images:\n",
        "        images = tf.convert_to_tensor(images)\n",
        "        image_features = image_features_extract_model(images)\n",
        "\n",
        "    return image_features, sequences"
      ],
      "metadata": {
        "id": "hdTxIGO-FnfN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# 훈련 데이터 및 검증 데이터 로드\n",
        "train_captions = load_captions_from_json(train_captions_folder)\n",
        "val_captions = load_captions_from_json(val_captions_folder)\n",
        "\n",
        "# 훈련 데이터셋 생성\n",
        "train_image_features, train_captions = create_dataset(train_images_folder, train_captions, image_features_extract_model)\n",
        "\n",
        "# 검증 데이터셋 생성\n",
        "val_image_features, val_captions = create_dataset(val_images_folder, val_captions, image_features_extract_model)\n",
        "\n",
        "# 모든 캡션을 하나의 리스트로 합치기\n",
        "all_captions = train_captions + val_captions\n",
        "\n",
        "# 메모리 정리\n",
        "K.clear_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr-f4BWKnwjj",
        "outputId": "7252e349-ffbf-4758-e139-ad14e9cf2ef3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading captions from /content/drive/MyDrive/이미지 설명문 추출 및 생성용 한국형 비전 데이터/Training/원천데이터: 4844it [02:24, 33.41it/s]\n",
            "Loading captions from /content/drive/MyDrive/이미지 설명문 추출 및 생성용 한국형 비전 데이터/Validation/원천데이터: 605it [00:20, 30.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화와 패딩을 위한 Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "all_captions_texts = [caption['text'] for caption in all_captions if 'text' in caption and caption['text'].strip()]\n",
        "tokenizer.fit_on_texts(all_captions_texts)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "HnpZzQFEuABW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터에서 캡션 텍스트 추출 및 토큰화\n",
        "train_captions_seqs = tokenizer.texts_to_sequences([caption['text'] for caption in train_captions if 'text' in caption and caption['text'].strip()])\n",
        "val_captions_seqs = tokenizer.texts_to_sequences([caption['text'] for caption in val_captions if 'text' in caption and caption['text'].strip()])\n",
        "\n",
        "# 훈련 데이터셋과 검증 데이터셋 중 비어있는 것이 있는지 확인\n",
        "if not train_captions_seqs or not val_captions_seqs:\n",
        "    raise ValueError(\"훈련 데이터셋 또는 검증 데이터셋이 비어있습니다.\")\n",
        "\n",
        "# 토큰화된 캡션을 정수 시퀀스로 변환\n",
        "max_length = max(len(seq) for seq in train_captions_seqs + val_captions_seqs)\n",
        "train_captions_padded = pad_sequences(train_captions_seqs, maxlen=max_length, padding='post')\n",
        "val_captions_padded = pad_sequences(val_captions_seqs, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "_Hu0rioOuCGL",
        "outputId": "5dafcdfc-e134-4687-c440-1d9f7f776fe7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-4c3ea0de5c9c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 훈련 데이터셋과 검증 데이터셋 중 비어있는 것이 있는지 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrain_captions_seqs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval_captions_seqs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"훈련 데이터셋 또는 검증 데이터셋이 비어있습니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 토큰화된 캡션을 정수 시퀀스로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 훈련 데이터셋 또는 검증 데이터셋이 비어있습니다."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지와 캡션 시퀀스를 결합하는 모델 정의\n",
        "\n",
        "embed_dim = 256\n",
        "units = 512"
      ],
      "metadata": {
        "id": "5oLZrFphiTA7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 특성을 처리하는 부분\n",
        "image_input = Input(shape=(train_image_features.shape[1],))\n",
        "image_dense = Dense(embed_dim, activation='relu')(image_input)\n",
        "\n",
        "# 시퀀스를 처리하는 부분\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_embed = Embedding(vocab_size, embed_dim, input_length=max_length)(caption_input)\n",
        "caption_lstm = LSTM(units, return_sequences=True)(caption_embed)\n",
        "\n",
        "# 이미지와 캡션 시퀀스를 결합\n",
        "combined = tf.keras.layers.Add()([image_dense, caption_lstm])\n",
        "combined = Dense(units, activation='relu')(combined)\n",
        "outputs = Dense(vocab_size, activation='softmax')(combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "QDVlXm4uLKph",
        "outputId": "0d2318b0-93b5-4108-b8f4-5b64aae50c2b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-93178213260a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#이미지와 캡션 시퀀스를 결합하는 모델 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivatioin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 모델 정의\n",
        "model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 요약\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1B3RhzRiMq0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}